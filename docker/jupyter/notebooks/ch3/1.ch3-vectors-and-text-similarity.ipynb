{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Chapter 3] Vectors and Text Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 3.1\n",
    "In this example, we explore Ranking two documents for the query \"Apple Juice\". We present the query as a feature vector, as well as the documents.\n",
    "\n",
    "#### Text Content\n",
    "*Query*: \"Apple Juice\"\n",
    "\n",
    "*Document 1*: \n",
    "```Lynn: ham and cheese sandwhich, chocolate cookie, ice water.\n",
    "Brian: turkey avocado sandwhich, plain potato chips, apple juice\n",
    "Mohammed: grilled chicken salad, fruit cup, lemonade```\n",
    "\n",
    "*Document 2*: ```Orchard Farms apple juice is premium, organic apple juice  made from the freshest apples and never from concentrate. It has received the regional award for best apple juice three years in a row.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense Vectors\n",
    "If we consider a vector with each keyword as a feature (48 terms total):\n",
    "```[a, and, apple, apples, avocado, award, best, brian, cheese, chicken, chips, chocolate, concentrate, cookie, cup, farms, for, freshest, from, fruit, grilled, ham, has, ice, in, is, it, juice, lemonade, lynn, made, mohammed, never, orchard, organic, plain, potato, premium, received, regional, row, salad, sandwhich, the, three, turkey, water, years]```\n",
    "\n",
    "\n",
    "Then our query becomes the 48-feature vector, where the `apple` and `juice` features both exist:\n",
    "Query:      ```[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding vectors for our documents are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1_vector = np.array([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0])\n",
    "doc2_vector = np.array([1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity\n",
    "To rank our documents, we then just need to calculate the cosine between each document and the query, \n",
    "which will become the relevance score for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(vector1,vector2):\n",
    "  return dot(vector1, vector2)/(norm(vector1)*norm(vector2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc1_vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-635d2e0c8179>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdoc1_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcos_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc1_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdoc2_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcos_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc2_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'doc1_vector' is not defined"
     ]
    }
   ],
   "source": [
    "doc1_score = cos_sim(query_vector, doc1_vector)\n",
    "doc2_score = cos_sim(query_vector, doc2_vector)\n",
    "\n",
    "print(\"Relevance Scores:\\n doc1: \" + str(doc1_score) + \"\\n doc2: \" + str(doc2_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting... Both documents received exactly the same relevance score, even though the documents contain lengthy vectors with very different content. It might not be immediately obvious, but let's simplify the calculation by focusing only on the features that matter.\n",
    "\n",
    "#### Sparse Vectors\n",
    "The key to understanding the calculation is understanding that the only features that matter are the ones shared between the query and a document. All other features (words appearing in documents that don't match the query) have zero impact on whether one document is ranked higher than another. As such, we can simplify our calculations significantly by creating sparse vectors that only include the terms present in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance Scores:\n",
      " doc1: 0.9999999999999998\n",
      " doc2: 0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "sparse_query_vector = [1, 1] #[apple, juice]\n",
    "sparse_doc1_vector = [1, 1]\n",
    "sparse_doc2_vector = [1, 1]\n",
    "\n",
    "doc1_score = cos_sim(sparse_query_vector, sparse_doc1_vector)\n",
    "doc2_score = cos_sim(sparse_query_vector, sparse_doc2_vector)\n",
    "\n",
    "print(\"Relevance Scores:\\n doc1: \" + str(doc1_score) + \"\\n doc2: \" + str(doc2_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, you'll notice several very interesting things:\n",
    "1. This simplified sparse vector calculation still shows both `doc1` and `doc2` returning equivalent relevance scores, since they both match all the words in the query.\n",
    "2. Even though the absolute score between the dense vector similarity (0.282842712474619) and the sparse vector similarity (0.9999999999999998) are different due to normalization, the scores are still the same relative to each other (equal to each other in this case).\n",
    "3. The feature weights for the two query terms (`apple`, `juice`) are exactly the same between the query and each of the documents, resulting in a cosine score of 1.0.\n",
    "\n",
    "The problem here, of course, is that the features in the vector only signifies IF the word `apple` or `juice` exists, not how well each document actually represents either of the terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance Scores:\n",
      " doc1: 0.9899494936611665\n",
      " doc2: 1.0\n"
     ]
    }
   ],
   "source": [
    "doc1_tf_vector = [1, 1] #[apple:1, juice:1]\n",
    "doc2_tf_vector = [3, 4] #[apple:3, juice:4]\n",
    "\n",
    "#query should represent the \"best possible\" match, so we include the \"top possible score\" for each term in the query vector.\n",
    "#we could alternatively normalize the scores in the documents\n",
    "query_vector = np.maximum.reduce([doc1_tf_vector, doc2_tf_vector]) #[3, 4]\n",
    "\n",
    "doc1_score = cos_sim(query_vector, doc1_tf_vector)\n",
    "doc2_score = cos_sim(query_vector, doc2_tf_vector)\n",
    "\n",
    "print(\"Relevance Scores:\\n doc1: \" + str(doc1_score) + \"\\n doc2: \" + str(doc2_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Frequency\n",
    "In Section 3.1.? we learn about Term Frequency (TF). The following example demonstrates how term frequency helps with our text-based sparse vector similarity scoring.\n",
    "\n",
    "*Document 1:* ```The interesting thing is that the person in the wrong made the right decision in the end.```\n",
    "\n",
    "*Document 2:* ```My favorite book is the cat in the hat, which is about a crazy cat who breaks into a house creates a crazy afternoon for two kids.```\n",
    "\n",
    "*Document 3:* ```My neighbors let the stray cat stay in their garage, which resulted in my favorite hat that I let them borrow being ruined.```\n",
    "\n",
    "Let's map these into their corresponding (sparse) vector representations and calculate a similarity score:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_counts = {\"doc1\": {\"the\": 5, \"cat\": 0, \"in\": 2, \"hat\": 0},\n",
    "          \"doc2\": {\"the\": 2, \"cat\": 2, \"in\": 1, \"hat\": 1},\n",
    "          \"doc3\": {\"the\": 1, \"cat\": 1, \"in\": 1, \"hat\": 1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc1_vector: [5, 0, 2, 5, 0]\n",
      "doc2_vector: [2, 2, 1, 2, 1]\n",
      "doc3_vector: [1, 1, 1, 1, 1]\n",
      "\n",
      "Relevance Scores:\n",
      " doc1: 0.956689206214921\n",
      " doc2: 0.9394501508629485\n",
      " doc3: 0.8733337646093731\n"
     ]
    }
   ],
   "source": [
    "#[the, cat, in, the, hat]\n",
    "doc1_tf_vector = [term_counts[\"doc1\"][\"the\"], term_counts[\"doc1\"][\"cat\"], term_counts[\"doc1\"][\"in\"], term_counts[\"doc1\"][\"the\"], term_counts[\"doc1\"][\"hat\"]]\n",
    "doc2_tf_vector = [term_counts[\"doc2\"][\"the\"], term_counts[\"doc2\"][\"cat\"], term_counts[\"doc2\"][\"in\"], term_counts[\"doc2\"][\"the\"], term_counts[\"doc2\"][\"hat\"]]\n",
    "doc3_tf_vector = [term_counts[\"doc3\"][\"the\"], term_counts[\"doc3\"][\"cat\"], term_counts[\"doc3\"][\"in\"], term_counts[\"doc3\"][\"the\"], term_counts[\"doc3\"][\"hat\"]]\n",
    "\n",
    "print (\"doc1_vector: [\" + \", \".join(map(str,doc1_tf_vector)) + \"]\")\n",
    "print (\"doc2_vector: [\" + \", \".join(map(str,doc2_tf_vector)) + \"]\")\n",
    "print (\"doc3_vector: [\" + \", \".join(map(str,doc3_tf_vector)) + \"]\\n\")\n",
    "                   \n",
    "#query vector contains the max value for each term, since this yields the highest similarity score\n",
    "query_vector = np.maximum.reduce([doc1_tf_vector, doc2_tf_vector, doc3_tf_vector]) # [5, 2, 2, 5, 1]\n",
    "\n",
    "doc1_score = cos_sim(query_vector, doc1_tf_vector)\n",
    "doc2_score = cos_sim(query_vector, doc2_tf_vector)\n",
    "doc3_score = cos_sim(query_vector, doc3_tf_vector)\n",
    "\n",
    "print(\"Relevance Scores:\\n doc1: \" + str(doc1_score) + \"\\n doc2: \" + str(doc2_score)+ \"\\n doc3: \" + str(doc3_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we at least receive different relevance scores now for each document based upon the number of times each term matches, the ordering of the results doesn't necessarily match our intuition about which documents are the best matches.\n",
    "\n",
    "Intuitively, we would instead expect the following ordering:\n",
    "1. doc2 (is about the book _The Cat in the Hat_ )\n",
    "2. doc3 (matches all of the words `the`, `cat`, `in`, and `hat`\n",
    "3. doc1 (only matches the words `the` and `in`, even though it contains them many times).\n",
    "\n",
    "The problem here, of course, is that since every occurrence of any word is considered just as important, the more times ANY term appears, the more relevant that document becomes. In this case, *doc1* is getting the highest score, because it contains 12 total term matches (`the` ten times, `in` two times), which more total term matches than any other document.\n",
    "\n",
    "Your intuition is probably screaming right \"Yeah, but nobody really cares about the words `the` and `in`. It's obvious that the words `cat` and `hat` should be given the most weight here!\"\n",
    "\n",
    "And you would be right. Let's modify our scoring calculation to fix this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency (IDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Document Frequency (DF)* for a term is defined as the total number of document in the search engine that contain the term, and it serve as a good measure for how important a term is. The intuition here is that more specific or rare words (like `cat` and `hat`) tend to be more important than more common words (like `the` and `in`).\n",
    "\n",
    "$$DF(t\\ in\\ d)=\\sum_{d\\ in\\ c} d.contains(t)\\ ?\\ 1\\ :\\ 0$$\n",
    "\n",
    "Since we would like words which are more important to get a higher score, we take an inverse of the document frequency (IDF), typically defined through the following function:\n",
    "\n",
    "$$IDF(t\\ in\\ d)=1 + log (\\ totalDocs\\ /\\ (\\ DF(t)\\ +\\ 1\\ )\\ )$$\n",
    "\n",
    "In our query for `the cat in the hat`, a vector of IDFs would thus look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idf_vector: [-0.09861228866810978, 0.7123179275482191, 0.4891743762340093, -0.09861228866810978, 1.0]\n"
     ]
    }
   ],
   "source": [
    "#[the, cat, in, hat]\n",
    "df_map = {\"the\": 8, \"cat\": 3, \"in\":4, \"hat\":2}\n",
    "totalDocs = 3\n",
    "\n",
    "def idf(term):\n",
    "    return 1 + np.log(totalDocs / (df_map[term] + 1) )\n",
    "\n",
    "#same for both queries and documents; IDF is term-dependent, not document dependent\n",
    "idf_vector = np.array([idf(\"the\"), idf(\"cat\"), idf(\"in\"), idf(\"the\"), idf(\"hat\")])\n",
    "\n",
    "print (\"idf_vector: [\" + \", \".join(map(str,idf_vector)) + \"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "We now have the two principle components of text-based relevance ranking:\n",
    "- TF (measures how well a term describes a document)\n",
    "- IDF (measures how important each term is)\n",
    "\n",
    "Most search engines, and many other data science applications, leverage a combination of each of these factors as the basis for textual similarity scoring, using the following function:\n",
    "\n",
    "$$TF\\_IDF = TF * IDF^2$$\n",
    "\n",
    "With this formula in place, we can finally calculate a relevance score (that weights both number of occurrences and usefulness of terms) for how well each of our documents match our query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(term_count):\n",
    "    return np.sqrt(term_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance Scores:\n",
      " doc1: [0.2507894754780836]\n",
      " doc2: [2.2176263779920133]\n",
      " doc3: [2.0042677264460087]\n"
     ]
    }
   ],
   "source": [
    "#[the, cat, in, the, hat]\n",
    "query_tfidf = [tf(1)*idf(\"the\"), tf(1)*idf(\"cat\"), tf(1)*idf(\"in\"), tf(1)*idf(\"the\"), tf(1)*(idf(\"hat\"))]\n",
    "\n",
    "doc1_tfidf = [\n",
    "               tf(term_counts[\"doc1\"][\"the\"]) * idf(\"the\") + \n",
    "               tf(term_counts[\"doc1\"][\"cat\"]) * idf(\"cat\") +\n",
    "               tf(term_counts[\"doc1\"][\"in\"]) * idf(\"in\") +\n",
    "               tf(term_counts[\"doc1\"][\"the\"]) * idf(\"the\") +\n",
    "               tf(term_counts[\"doc1\"][\"hat\"]) * idf(\"hat\")]\n",
    "\n",
    "doc2_tfidf = [\n",
    "               tf(term_counts[\"doc2\"][\"the\"]) * idf(\"the\") + \n",
    "               tf(term_counts[\"doc2\"][\"cat\"]) * idf(\"cat\") +\n",
    "               tf(term_counts[\"doc2\"][\"in\"]) * idf(\"in\") +\n",
    "               tf(term_counts[\"doc2\"][\"the\"]) * idf(\"the\") +\n",
    "               tf(term_counts[\"doc2\"][\"hat\"]) * idf(\"hat\")]\n",
    "\n",
    "doc3_tfidf = [\n",
    "               tf(term_counts[\"doc3\"][\"the\"]) * idf(\"the\") + \n",
    "               tf(term_counts[\"doc3\"][\"cat\"]) * idf(\"cat\") +\n",
    "               tf(term_counts[\"doc3\"][\"in\"]) * idf(\"in\") +\n",
    "               tf(term_counts[\"doc3\"][\"the\"]) * idf(\"the\") +\n",
    "               tf(term_counts[\"doc3\"][\"hat\"]) * idf(\"hat\")]\n",
    "\n",
    "print(\"Relevance Scores:\\n doc1: \" + str(doc1_tfidf) + \"\\n doc2: \" + str(doc2_tfidf) + \"\\n doc3: \" + str(doc3_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally!\n",
    "Finally our search results make intuitive sense! `doc2` gets the highest score, since it matches the most important words the most, followed by `doc3`, which contains all the words, but not as many times, followed by `doc1`, which only contains an abundance of insignificant words.\n",
    "\n",
    "This TF-IDF calculation is at the heart of many search engine relevance calculations, including the default algorithms - called BM25 - used by both Apache Solr and Elasticsearch. In addition, it is possible to match on much more thatn just text keywords - modern search engines enable dynamically specifying boosts of fields, terms, and functions, which enables full control over the relevance scoring calculation.\n",
    "\n",
    "We'll introduce each of these in the next workbook: [Controlling Relevance](2.ch3-controlling-relevance.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
